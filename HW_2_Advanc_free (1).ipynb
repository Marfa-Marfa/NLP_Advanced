{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "05de521d",
      "metadata": {
        "id": "05de521d"
      },
      "source": [
        "# Домашнее задание\n",
        "Дообучение энкодерных моделей\n",
        "\n",
        "**Цель:**\n",
        "\n",
        "В этом задании вы поработаете с энкодерными трансформерными моделями (например, BERT) и дообучите их для решения различных задач обработки естественного языка (NLP).\n",
        "\n",
        "**Описание / пошаговая инструкция выполнения домашнего задания:**\n",
        "\n",
        "\n",
        "1. **Обработка данных:**\n",
        "\n",
        "- В дополнительных материалах к уроку найдите датасет с отзывами о ресторанах (restaurants_reviews.jsonl).\n",
        "- Разбейте данные на train/val/test, отложив по 15% в test и val. Не забудьте зафиксировать random_state. В качестве целевой переменной возьмите общий отзыв из колонки general.\n",
        "- Оставьте только отзывы с рейтингом general равным 1, 3 и 5. Для удобства перекодируйте лейблы 1, 3 и 5 в метки 0, 1, 2.\n",
        "\n",
        "\n",
        "2. **Дообучение энкодерных моделей:**\n",
        "\n",
        "- Возьмите 3 модели:\n",
        "\n",
        "    - https://huggingface.co/sberbank-ai/ruBert-base/ или https://huggingface.co/sberbank-ai/ruBert-large/\n",
        "    - https://huggingface.co/cointegrated/rubert-tiny2\n",
        "    - https://huggingface.co/google-bert/bert-base-multilingual-cased\n",
        "\n",
        "- Дообучите каждую модель на train части данных. Обучение прекращайте, когда модель выходит на плато по метрике на валидации.\n",
        "- Возьмите итоговый чекпоинт (версию, с минимальным loss на валидации) и замерьте качество на test. В качестве метрики используйте accuracy.\n",
        "\n",
        "\n",
        "3. **Анализ результатов:**\n",
        "\n",
        "- Составьте таблицу с результатами для каждой модели, включающую:\n",
        "\n",
        "    - количество эпох до достижения минимального значения loss на валидационной выборке;\n",
        "    - время, затрачиваемое на одну итерацию обучения;\n",
        "    - общее время дообучения;\n",
        "\n",
        "- Проведите анализ полученных результатов и опишите выводы в Markdown в ноутбуке.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yaX_DMv91GiU",
      "metadata": {
        "id": "yaX_DMv91GiU"
      },
      "outputs": [],
      "source": [
        "import gc, time\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 300\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm, trange\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import DataCollatorWithPadding\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d0341a",
      "metadata": {
        "id": "c9d0341a"
      },
      "source": [
        "#1. Обработка данных\n",
        "\n",
        "##1.1.Загрузка данных: датасет с отзывами о ресторанах (restaurants_reviews.jsonl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4S_fGNW7xC11",
      "metadata": {
        "id": "4S_fGNW7xC11"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8173dad5",
      "metadata": {
        "id": "8173dad5"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_json('./restaurants_reviews.jsonl', lines=True)\n",
        "df = pd.read_json('/content/drive/MyDrive/restaurants_reviews.jsonl', lines=True)\n",
        "df[20:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef3783ab",
      "metadata": {
        "id": "ef3783ab"
      },
      "source": [
        "##1.2.Оставляем только отзывы с рейтингом \"general\" равным 1, 3 и 5, используем булевую маску (True/False), True для строк, где general равен 1, 3 или 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SPMxXPFtT8Am",
      "metadata": {
        "id": "SPMxXPFtT8Am"
      },
      "outputs": [],
      "source": [
        "df = df[df['general'].isin([1,3,5])] # перезапишем df, оставляя только строки с general равным 1, 3 или 5.\n",
        "df['general'].value_counts() # посмотрим распределение"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ed904e",
      "metadata": {
        "id": "11ed904e"
      },
      "source": [
        "## 1.3.Перекодируем лейблы 1, 3 и 5 в метки 0, 1, 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0_PnschPXxsb",
      "metadata": {
        "id": "0_PnschPXxsb"
      },
      "outputs": [],
      "source": [
        "rating_map = {1:0, 3:1, 5:2}\n",
        "df['general'] = df['general'].map(rating_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd410ed",
      "metadata": {
        "id": "bdd410ed"
      },
      "outputs": [],
      "source": [
        "df['general'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38f57d4",
      "metadata": {
        "id": "f38f57d4"
      },
      "source": [
        "##1.4.Разбиваем данные на train/val/test, отложив по 15% в test и val. В качестве целевой переменной берем общий отзыв из колонки general."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TM8s7b6po7cm",
      "metadata": {
        "id": "TM8s7b6po7cm"
      },
      "outputs": [],
      "source": [
        "# Создаём объект Dataset из библиотеки Hugging Face datasets на основе DataFrame df\n",
        "dataset_f = Dataset.from_dict({'text': df.text, 'label': df.general}) # целевая переменная - \"general\" (Y)\n",
        "dataset_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZsWgIHwMuBaW",
      "metadata": {
        "id": "ZsWgIHwMuBaW"
      },
      "outputs": [],
      "source": [
        "# Первичное разделение на обучающую + валидационную (85%) и тестовую (15%) части\n",
        "train_test_split = dataset_f.train_test_split(test_size=0.15, seed=42)  # Указываем seed для воспроизводимости\n",
        "train_val_split = train_test_split[\"train\"]  # Временный набор для дальнейшего деления\n",
        "test_dataset = train_test_split[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iTYp5EWQu_Zc",
      "metadata": {
        "id": "iTYp5EWQu_Zc"
      },
      "outputs": [],
      "source": [
        "# Вторичное разделение на финальные обучающую (70%) и валидационную (15%) части\n",
        "# Размер валидации: 0.15 / 0.85 ≈ 0.1765 от оставшихся данных\n",
        "final_split = train_val_split.train_test_split(test_size=0.1765, seed=42)\n",
        "train_dataset = final_split[\"train\"]\n",
        "val_dataset = final_split[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eBCotdDtxsOU",
      "metadata": {
        "id": "eBCotdDtxsOU"
      },
      "outputs": [],
      "source": [
        "print(\"Размеры после разделения средствами datasets:\")\n",
        "print(f\"  Обучающая выборка (train): {len(train_dataset)} примеров\")\n",
        "print(f\"  Валидационная выборка (validation): {len(val_dataset)} примеров\")\n",
        "print(f\"  Тестовая выборка (test): {len(test_dataset)} примеров\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6LItvo4cyEy2",
      "metadata": {
        "id": "6LItvo4cyEy2"
      },
      "outputs": [],
      "source": [
        "# Создаем удобный словарь датасетов\n",
        "data = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dfc28ef",
      "metadata": {
        "id": "7dfc28ef"
      },
      "outputs": [],
      "source": [
        "# Чистим память\n",
        "del dataset_f, train_test_split, train_val_split, final_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9773bf64",
      "metadata": {
        "id": "9773bf64"
      },
      "outputs": [],
      "source": [
        "data['train'][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c05f87",
      "metadata": {
        "id": "b4c05f87"
      },
      "source": [
        "# 2. Дообучение энкодерных моделей\n",
        "\n",
        "    - https://huggingface.co/sberbank-ai/ruBert-base/\n",
        "    - https://huggingface.co/cointegrated/rubert-tiny2\n",
        "    - https://huggingface.co/google-bert/bert-base-multilingual-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27706f81",
      "metadata": {
        "id": "27706f81"
      },
      "source": [
        "## 2.1 ruBERT-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45352532",
      "metadata": {
        "id": "45352532"
      },
      "outputs": [],
      "source": [
        "#Загрузка модели\n",
        "b_base_model = 'ai-forever/ruBert-base'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d71609a",
      "metadata": {
        "id": "6d71609a"
      },
      "outputs": [],
      "source": [
        "#Загрузка токенизатора\n",
        "tokenizer = AutoTokenizer.from_pretrained(b_base_model)\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab54cdc2",
      "metadata": {
        "id": "ab54cdc2"
      },
      "outputs": [],
      "source": [
        "#Подготовка текстовых данных для моделей трансформеров.Токенизация текстовых данных с помощью библиотеки Transformers\n",
        "data_tokenized = data.map(lambda x: tokenizer(x['text'], truncation=True, max_length=512), batched=True, remove_columns=['text'])\n",
        "data_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79806fba",
      "metadata": {
        "id": "79806fba"
      },
      "outputs": [],
      "source": [
        "#Убедимся, что токенизация работает правильно: выведем третий элемент из тренировочной части токенизированного датасета\n",
        "print(data_tokenized['train'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f479ec0c",
      "metadata": {
        "id": "f479ec0c"
      },
      "outputs": [],
      "source": [
        "# Cоздаём объект DataCollatorWithPadding для автоматического дополнения (padding) батчей данных до одинаковой длины.\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e672c6e6",
      "metadata": {
        "id": "e672c6e6"
      },
      "outputs": [],
      "source": [
        "#Создаём три DataLoader'а (для загруки данных батчами, их автоматического перемешивания и коллатора)для обучения, валидации и тестирования модели\n",
        "train_dataloader = DataLoader(data_tokenized['train'], shuffle=True, batch_size=8, collate_fn=collator)\n",
        "val_dataloader = DataLoader(data_tokenized['validation'], shuffle=False, batch_size=8, collate_fn=collator)\n",
        "test_dataloader = DataLoader(data_tokenized['test'], shuffle=False, batch_size=8, collate_fn=collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1c32f5",
      "metadata": {
        "id": "3d1c32f5"
      },
      "outputs": [],
      "source": [
        "#Создаём модель для классификации последовательностей текста на основе предобученной модели\n",
        "model = AutoModelForSequenceClassification.from_pretrained(b_base_model, num_labels=3)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5d6b26-083a-4c3b-b9c3-32b0c0b261f2",
      "metadata": {
        "id": "1e5d6b26-083a-4c3b-b9c3-32b0c0b261f2"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb8f583e-7f75-4f14-982a-be9837485982",
      "metadata": {
        "id": "cb8f583e-7f75-4f14-982a-be9837485982"
      },
      "outputs": [],
      "source": [
        "#Перемещаем модель на выбранное вычислительное устройство (GPU или CPU)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa48ad2",
      "metadata": {
        "id": "bfa48ad2"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-6)  #малая скорость обучения и след. меньше риск \"перепрыгнуть\" оптимум\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c4bc504-30b0-429c-ac96-b523eb1f84a9",
      "metadata": {
        "id": "7c4bc504-30b0-429c-ac96-b523eb1f84a9"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87cb6470-d561-4928-995e-f054162b146a",
      "metadata": {
        "id": "87cb6470-d561-4928-995e-f054162b146a"
      },
      "outputs": [],
      "source": [
        "#Полный цикл обучения нейронной сети с сохранением лучшей модели\n",
        "best_eval_loss = float('inf') #начальное значение для поиска минимума\n",
        "\n",
        "losses = []\n",
        "epoch_train_loss = []\n",
        "epoch_eval_loss = []\n",
        "epoch_train_time = []\n",
        "train_time = []\n",
        "start = time.time()\n",
        "for epoch in trange(20):\n",
        "    pbar = tqdm(train_dataloader)\n",
        "    model.train()\n",
        "    for i, batch in enumerate(pbar):\n",
        "        out = model(**batch.to(model.device))\n",
        "        out.loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        losses.append(out.loss.item())\n",
        "        train_time.append(time.time() - start)\n",
        "        pbar.set_description(f'loss: {np.mean(losses[-100:]):2.2f}')\n",
        "    epoch_train_loss.append(np.mean(losses[-100:]))\n",
        "\n",
        "    model.eval()\n",
        "    eval_losses = []\n",
        "    eval_preds = []\n",
        "    eval_targets = []\n",
        "    val_time = []\n",
        "    for batch in tqdm(val_dataloader):\n",
        "        with torch.no_grad():\n",
        "                out = model(**batch.to(model.device))\n",
        "        eval_losses.append(out.loss.item())\n",
        "        eval_preds.extend(out.logits.argmax(1).tolist())\n",
        "        eval_targets.extend(batch['labels'].tolist())\n",
        "        val_time.append(time.time() - start)\n",
        "    epoch_eval_loss.append(np.mean(eval_losses))\n",
        "    epoch_train_time.append(elapsed := time.time() - start)\n",
        "    val_loss = np.mean(eval_losses)\n",
        "    print('Epoch:', epoch+1, 'Train Loss', np.mean(losses[-100:]), 'Eval Loss', val_loss, 'Accuracy', np.mean(np.array(eval_targets) == eval_preds), 'Time:', elapsed)\n",
        "\n",
        "    #сохраняем лучшую модель\n",
        "    if val_loss < best_eval_loss:\n",
        "        best_eval_loss = val_loss\n",
        "        torch.save(model.state_dict(), model.name_or_path.split('/')[1]+'.saved.weights.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c4616c-90ba-4b60-9e05-0300da795021",
      "metadata": {
        "id": "17c4616c-90ba-4b60-9e05-0300da795021"
      },
      "outputs": [],
      "source": [
        "#Находим индекс (номер) лучшей эпохи обучения и соответствующую минимальную ошибку валидации\n",
        "epoch_eval_loss.index(min(epoch_eval_loss)), min(epoch_eval_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0022a1-dd6c-4bc9-b1d0-8646ff9d152b",
      "metadata": {
        "id": "ac0022a1-dd6c-4bc9-b1d0-8646ff9d152b"
      },
      "outputs": [],
      "source": [
        "#Находим общее время обучения до лучшей эпохи (эпохи с минимальной ошибкой валидации)\n",
        "epoch_train_time[epoch_eval_loss.index(min(epoch_eval_loss))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cceec43-fcbb-40d3-8d55-3cfba2bfcf07",
      "metadata": {
        "id": "9cceec43-fcbb-40d3-8d55-3cfba2bfcf07"
      },
      "outputs": [],
      "source": [
        "epoch_train_time[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70b30f9-0c48-4556-bb68-5b0f8e8263f2",
      "metadata": {
        "id": "c70b30f9-0c48-4556-bb68-5b0f8e8263f2"
      },
      "outputs": [],
      "source": [
        "#вычисление среднего времени обучения на одну эпоху\n",
        "epoch_train_time[-1]/len(epoch_train_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sNAkQXdUVLa_",
      "metadata": {
        "id": "sNAkQXdUVLa_"
      },
      "outputs": [],
      "source": [
        "# Построение графика потерь на обучающей и валидационной выборках\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(np.arange(1, 21, 1), epoch_train_loss, color='darkblue', linewidth=2, label='Train Loss')\n",
        "plt.plot(np.arange(1, 21, 1), epoch_eval_loss, color='red', linewidth=2, label='Eval Loss', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(1, 21, 1))\n",
        "plt.xlabel('Эпоха', fontsize=12)\n",
        "plt.ylabel('Потери', fontsize=12)\n",
        "plt.title('Динамика потерь по эпохам', fontsize=14, pad=15)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()  # Автоматическая подгонка отступов\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "772153c4-11b6-471c-8944-be8f0ad838b9",
      "metadata": {
        "id": "772153c4-11b6-471c-8944-be8f0ad838b9"
      },
      "source": [
        "##Загрузим лучшую сохраненную модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041a311e-d32e-410e-a2ab-6040bbd68ee7",
      "metadata": {
        "id": "041a311e-d32e-410e-a2ab-6040bbd68ee7"
      },
      "outputs": [],
      "source": [
        "#Загрузка лучшей сохраненной модели\n",
        "path = model.name_or_path.split('/')[1]+'.saved.weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77269b9e-a994-44f0-adb1-583785488f75",
      "metadata": {
        "id": "77269b9e-a994-44f0-adb1-583785488f75"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13f1c71b-9a3b-43a1-a926-53f3eeda9389",
      "metadata": {
        "id": "13f1c71b-9a3b-43a1-a926-53f3eeda9389"
      },
      "outputs": [],
      "source": [
        "#Выполняем оценку модели на тестовом наборе данных.\n",
        "model.eval()\n",
        "test_losses = []\n",
        "test_preds = []\n",
        "test_targets = []\n",
        "\n",
        "for batch in tqdm(test_dataloader):\n",
        "    with torch.no_grad():\n",
        "            out = model(**batch.to(model.device))\n",
        "    test_losses.append(out.loss.item())\n",
        "    test_preds.extend(out.logits.argmax(1).tolist())\n",
        "    test_targets.extend(batch['labels'].tolist())\n",
        "\n",
        "print('Eval Loss', np.mean(test_losses), 'Accuracy', np.mean(np.array(test_targets) == test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3b5447-2798-4945-b37c-6a1561d425db",
      "metadata": {
        "id": "0d3b5447-2798-4945-b37c-6a1561d425db"
      },
      "outputs": [],
      "source": [
        "accuracy_score(test_targets, test_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "301d2f5b-7ad7-4ad9-ae4b-83c95ba57ae2",
      "metadata": {
        "id": "301d2f5b-7ad7-4ad9-ae4b-83c95ba57ae2"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(test_targets, test_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56783f26-2a18-474b-a903-d6bd5505a537",
      "metadata": {
        "id": "56783f26-2a18-474b-a903-d6bd5505a537"
      },
      "source": [
        "## Оценка модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec857f7",
      "metadata": {
        "id": "dec857f7"
      },
      "outputs": [],
      "source": [
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5ca46e-0297-4a05-a972-f5e8941a610e",
      "metadata": {
        "id": "7a5ca46e-0297-4a05-a972-f5e8941a610e"
      },
      "outputs": [],
      "source": [
        "def quality(etime, vloss):\n",
        "    \"\"\"\n",
        "        etime: списокаккумулированного времени train для каждой эпохи;\n",
        "        vloss: список validation losses для каждой эпохи;\n",
        "    \"\"\"\n",
        "    # количество эпох до достижения минимального значения loss на валидационной выборке,\n",
        "    min_epoch_num = vloss.index(min(vloss))\n",
        "    #общее время дообучения\n",
        "    total_train_time = etime[min_epoch_num]\n",
        "    #время, затрачиваемое на одну итерацию обучения\n",
        "    avg_train_step_time = etime[-1]/len(etime)\n",
        "    print(f\"min_epoch_num: {min_epoch_num+1}, avg_train_step_time: {avg_train_step_time}, total_train_time: {total_train_time} \")\n",
        "    return [min_epoch_num+1, round(avg_train_step_time, 2), round(total_train_time, 2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99518e4-0733-4dfa-a53c-483b9d7c2487",
      "metadata": {
        "id": "d99518e4-0733-4dfa-a53c-483b9d7c2487"
      },
      "outputs": [],
      "source": [
        "def get_model_results(etime, vloss, dataloader, model):\n",
        "    \"\"\"\n",
        "        dataloader: Dataloader для  модели;\n",
        "        model: Model для test;\n",
        "    \"\"\"\n",
        "    # количество эпох до достижения минимального значения loss на валидационной выборке,\n",
        "    min_epoch_num = vloss.index(min(vloss))\n",
        "    #общее время дообучения\n",
        "    total_train_time = etime[min_epoch_num]\n",
        "    #время, затрачиваемое на одну итерацию обучения\n",
        "    avg_train_step_time = etime[-1]/len(etime)\n",
        "\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    test_preds = []\n",
        "    test_targets = []\n",
        "\n",
        "    for batch in tqdm(test_dataloader):\n",
        "        with torch.no_grad():\n",
        "                out = model(**batch.to(model.device))\n",
        "        test_losses.append(out.loss.item())\n",
        "        test_preds.extend(out.logits.argmax(1).tolist())\n",
        "        test_targets.extend(batch['labels'].tolist())\n",
        "\n",
        "    accuracy_metric = np.mean(np.array(test_targets) == test_preds)\n",
        "\n",
        "    print(f\"min_epoch: {min_epoch_num+1}, epoch_time: {avg_train_step_time}, total_train_time: {total_train_time}, accuracy: {accuracy_metric}\")\n",
        "    return [min_epoch_num+1, round(avg_train_step_time, 2), round(total_train_time, 2), round(accuracy_metric,4)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b1c2baf-40fe-43d7-8b34-2444180f0df6",
      "metadata": {
        "id": "5b1c2baf-40fe-43d7-8b34-2444180f0df6"
      },
      "outputs": [],
      "source": [
        "quality(epoch_train_time, epoch_eval_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf72667-039a-48c4-9b2d-fe6e36718bf6",
      "metadata": {
        "id": "baf72667-039a-48c4-9b2d-fe6e36718bf6"
      },
      "outputs": [],
      "source": [
        "get_model_results(epoch_train_time, epoch_eval_loss, test_dataloader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e62479-a9fe-4264-ab53-e1ac8320b14b",
      "metadata": {
        "id": "31e62479-a9fe-4264-ab53-e1ac8320b14b"
      },
      "source": [
        "Сохраним результаты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1fd8fb-e11a-4497-b083-83e633fbb4bf",
      "metadata": {
        "id": "ac1fd8fb-e11a-4497-b083-83e633fbb4bf"
      },
      "outputs": [],
      "source": [
        "results['ruBERT-base'] = get_model_results(epoch_train_time, epoch_eval_loss, test_dataloader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d61b20-fff0-4ed3-8db1-ff218777bafb",
      "metadata": {
        "id": "48d61b20-fff0-4ed3-8db1-ff218777bafb"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(results, index=['Epoch num','Epoch avg time','Total train time','Accuracy']).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b5378c9",
      "metadata": {
        "id": "8b5378c9"
      },
      "source": [
        "## 2.2 ruBERT-tiny2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb2d3d8",
      "metadata": {
        "id": "feb2d3d8"
      },
      "outputs": [],
      "source": [
        "b_base_model = 'cointegrated/rubert-tiny2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b9e1076-d54e-43a1-9b4c-2abfa93f5400",
      "metadata": {
        "id": "9b9e1076-d54e-43a1-9b4c-2abfa93f5400"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(b_base_model)\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898d414e-cf76-4412-a50c-7dbdcbc704ba",
      "metadata": {
        "id": "898d414e-cf76-4412-a50c-7dbdcbc704ba"
      },
      "outputs": [],
      "source": [
        "data_tokenized = data.map(lambda x: tokenizer(x['text'], truncation=True, max_length=512), batched=True, remove_columns=['text'])\n",
        "data_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c16d7e8-ef6a-4a31-a38e-f3376a1559d0",
      "metadata": {
        "id": "0c16d7e8-ef6a-4a31-a38e-f3376a1559d0"
      },
      "outputs": [],
      "source": [
        "print(data_tokenized['train'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44022398-3b56-4116-b0ed-13d65a5a097d",
      "metadata": {
        "id": "44022398-3b56-4116-b0ed-13d65a5a097d"
      },
      "outputs": [],
      "source": [
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d9daeb9-dea4-4cdf-a5cc-941ec02b5f9e",
      "metadata": {
        "id": "1d9daeb9-dea4-4cdf-a5cc-941ec02b5f9e"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(data_tokenized['train'], shuffle=True, batch_size=16, collate_fn=collator)\n",
        "val_dataloader = DataLoader(data_tokenized['validation'], shuffle=False, batch_size=16, collate_fn=collator)\n",
        "test_dataloader = DataLoader(data_tokenized['test'], shuffle=False, batch_size=16, collate_fn=collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f00b4e",
      "metadata": {
        "id": "e6f00b4e"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(b_base_model, num_labels=3)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f02a8be-fa83-48b8-9197-357cc0e10d0d",
      "metadata": {
        "id": "9f02a8be-fa83-48b8-9197-357cc0e10d0d"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5437e56c-a7d4-4412-949f-10438863e572",
      "metadata": {
        "id": "5437e56c-a7d4-4412-949f-10438863e572"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac1c187-59a3-4f05-ab54-3861c98b1763",
      "metadata": {
        "id": "9ac1c187-59a3-4f05-ab54-3861c98b1763"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de4ee42-fdad-4df0-b826-0cfaa8f83d8e",
      "metadata": {
        "id": "9de4ee42-fdad-4df0-b826-0cfaa8f83d8e"
      },
      "outputs": [],
      "source": [
        "best_eval_loss = float('inf')\n",
        "\n",
        "losses = []\n",
        "epoch_train_loss = []\n",
        "epoch_eval_loss = []\n",
        "epoch_train_time = []\n",
        "train_time = []\n",
        "start = time.time()\n",
        "for epoch in trange(20):\n",
        "    pbar = tqdm(train_dataloader)\n",
        "    model.train()\n",
        "    for i, batch in enumerate(pbar):\n",
        "        out = model(**batch.to(model.device))\n",
        "        out.loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        losses.append(out.loss.item())\n",
        "        train_time.append(time.time() - start)\n",
        "        pbar.set_description(f'loss: {np.mean(losses[-100:]):2.2f}')\n",
        "    epoch_train_loss.append(np.mean(losses[-100:]))\n",
        "\n",
        "    model.eval()\n",
        "    eval_losses = []\n",
        "    eval_preds = []\n",
        "    eval_targets = []\n",
        "    val_time = []\n",
        "    for batch in tqdm(val_dataloader):\n",
        "        with torch.no_grad():\n",
        "                out = model(**batch.to(model.device))\n",
        "        eval_losses.append(out.loss.item())\n",
        "        eval_preds.extend(out.logits.argmax(1).tolist())\n",
        "        eval_targets.extend(batch['labels'].tolist())\n",
        "        val_time.append(time.time() - start)\n",
        "    epoch_eval_loss.append(np.mean(eval_losses))\n",
        "    epoch_train_time.append(elapsed := time.time() - start)\n",
        "    val_loss = np.mean(eval_losses)\n",
        "    print('Epoch:', epoch+1, 'Train Loss', np.mean(losses[-100:]), 'Eval Loss', val_loss, 'Accuracy', np.mean(np.array(eval_targets) == eval_preds), 'Time:', elapsed)\n",
        "\n",
        "    #сохранение лучшей модели\n",
        "    if val_loss < best_eval_loss:\n",
        "        best_eval_loss = val_loss\n",
        "        torch.save(model.state_dict(), model.name_or_path.split('/')[1]+'.saved.weights.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WqgSxI2OZceH",
      "metadata": {
        "id": "WqgSxI2OZceH"
      },
      "outputs": [],
      "source": [
        "# Построение графика потерь на обучающей и валидационной выборках\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(np.arange(1, 21, 1), epoch_train_loss, color='darkblue', linewidth=2, label='Train Loss')\n",
        "plt.plot(np.arange(1, 21, 1), epoch_eval_loss, color='red', linewidth=2, label='Eval Loss', linestyle='--')\n",
        "plt.xticks(np.arange(1, 21, 1))\n",
        "plt.xlabel('Эпоха', fontsize=12)\n",
        "plt.ylabel('Потери', fontsize=12)\n",
        "plt.title('Динамика потерь по эпохам', fontsize=14, pad=15)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()  # Автоматическая подгонка отступов\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab6040c0-7301-49de-a3eb-1be713275099",
      "metadata": {
        "id": "ab6040c0-7301-49de-a3eb-1be713275099"
      },
      "source": [
        "##Загрузим лучшую сохраненную модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b42b4ca-7e3f-4458-9242-11c10730c38d",
      "metadata": {
        "id": "2b42b4ca-7e3f-4458-9242-11c10730c38d"
      },
      "outputs": [],
      "source": [
        "#loading the best saved model\n",
        "path = model.name_or_path.split('/')[1]+'.saved.weights.pt'\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d37c42-5ade-4363-8b54-2eaad0987e80",
      "metadata": {
        "id": "f0d37c42-5ade-4363-8b54-2eaad0987e80"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72be85e8-f81f-46c3-a615-99b56b59b130",
      "metadata": {
        "id": "72be85e8-f81f-46c3-a615-99b56b59b130"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6093b8-d7c2-49c9-8cf0-4f19f8e18869",
      "metadata": {
        "id": "db6093b8-d7c2-49c9-8cf0-4f19f8e18869"
      },
      "source": [
        "### Протестируем и сохраним результаты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53b2390-9376-47a0-9434-8394e63e9c2b",
      "metadata": {
        "id": "e53b2390-9376-47a0-9434-8394e63e9c2b"
      },
      "outputs": [],
      "source": [
        "get_model_results(epoch_train_time, epoch_eval_loss, test_dataloader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411022b5-63cb-469b-b147-5eb6134d12b8",
      "metadata": {
        "id": "411022b5-63cb-469b-b147-5eb6134d12b8"
      },
      "outputs": [],
      "source": [
        "results['ruBERT-tiny2'] = get_model_results(epoch_train_time, epoch_eval_loss, test_dataloader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff1d38c",
      "metadata": {
        "id": "3ff1d38c"
      },
      "source": [
        "## 3. Анализ результатов"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ebb44a",
      "metadata": {
        "id": "01ebb44a"
      },
      "source": [
        "Расчет модели bert-base-multilingual-cased не был выполнен из-за того, что закончились вычислительные ресурсы. Но это не мешает сделать итоговые выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a12c79-146e-4a34-9f65-7da6410e744d",
      "metadata": {
        "id": "05a12c79-146e-4a34-9f65-7da6410e744d"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(results, index = ['Epoch num','Epoch avg time','Total train time','Accuracy']).T.sort_values(by=['Accuracy'], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db7bcfe-6ba4-4b6e-9232-70076236b5cd",
      "metadata": {
        "id": "9db7bcfe-6ba4-4b6e-9232-70076236b5cd"
      },
      "source": [
        "**Вывод:**\n",
        "\n",
        "между моделями наблюдается компромисс между точностью и скоростью. Выбор лучшей модели будет зависеть от конкретных целей проекта.\n",
        "\n",
        "**Точность:**\n",
        "\n",
        "- ruBert-base точнее (0.90). Разница в 2% в классификации бывает важной и может оправдать использование более тяжелой модели (670 МБ).\n",
        "- rubert-tiny2 показывает отличный результат для своего класса (точность =0.88, размер модели около 125МБ). Уступая всего 2%, она показывает высокую эффективность архитектуры, адаптированной для русского языка.\n",
        "\n",
        "**Скорость и эффективность обучения**\n",
        "\n",
        "Здесь преимущество rubert-tiny2 абсолютно очевидно:\n",
        "\n",
        "- Время эпохи меньше в ~11.5 раз (12 сек. против 127 сек.). Это позволяет гораздо быстрее проводить эксперименты, отлаживать код и перебирать гиперпараметры.\n",
        "- Общее время обучения меньше в ~7,7 раз (70 сек. против 536 сек.). Tiny2 достигла плато за большее число эпох, но благодаря скорости каждой эпохи обучилась значительно быстрее.\n",
        "- ruBert-base достигла плато быстрее (за 4 эпохи), но цена каждой эпохи очень высока.\n",
        "\n",
        "Для предварительных расчетов в Google Colab я выбрала бы модель tiny2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "sCqyayCUu7WE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCqyayCUu7WE",
        "outputId": "9bb409cf-cddb-48df-a3d0-add288300d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "gl9EZIta4MqZ",
      "metadata": {
        "id": "gl9EZIta4MqZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea0b91b-189b-4562-fb0a-7730d2603786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/HW_2_Advanc_free.ipynb to notebook\n",
            "[NbConvertApp] Writing 35550 bytes to /content/drive/MyDrive/HW_2_Advanc_free.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Установим nbconver\n",
        "!pip install nbconvert -q\n",
        "\n",
        "file_path = '/content/drive/MyDrive/HW_2_Advanc_free.ipynb'\n",
        "\n",
        "\n",
        "# \"очистим\" блокнот, удалив выводы и проблемные метаданные\n",
        "!jupyter nbconvert \"{file_path}\" --to notebook --output \"{file_path}\" --clear-output --inplace"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}